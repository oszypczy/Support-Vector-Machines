{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset(input_file: str) -> tuple:\n",
    "\n",
    "    dataframe = pd.read_csv(input_file)\n",
    "    dataframe  = pd.get_dummies(dataframe ,\n",
    "                                columns=['gender',\n",
    "                                         'race/ethnicity',\n",
    "                                         \"parental level of education\",\n",
    "                                         \"lunch\",\n",
    "                                         \"test preparation course\"],\n",
    "                                prefix=['gender',\n",
    "                                        'group',\n",
    "                                        'parent_ed',\n",
    "                                        'lunch',\n",
    "                                        'prep_course'], dtype=int)\n",
    "\n",
    "    # Convert score columns to integers\n",
    "    dataframe[['math score', 'reading score', 'writing score']] = dataframe[['math score', 'reading score', 'writing score']].astype(int)\n",
    "\n",
    "    # Create a new column 'passed' based on your condition\n",
    "    dataframe['passed'] = dataframe.apply(lambda row: 1 if (row['math score'] > 60) & (row['reading score'] > 60) & (row['writing score'] > 60) else -1, axis=1)\n",
    "    dataframe.drop(['math score', 'reading score', 'writing score'], axis=1, inplace=True)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    y = dataframe.loc[:, 'passed']\n",
    "    x_train, x_test, y_train, y_test = tts(dataframe, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Convert the dataset into numpy arrays\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1 : np.ndarray, x2 : np.ndarray) -> np.ndarray:\n",
    "    return np.dot(x1, x2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1 : np.ndarray, x2 : np.ndarray) -> np.ndarray:\n",
    "    sigma = 1.0\n",
    "    return np.exp((np.linalg.norm(x1 - x2) ** 2) / (-2 * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, kernel, lambda_ = 0.5):\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        self.kernel = kernel\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    # Hinge Loss Function / Calculation\n",
    "    def hingeloss(self, w, b, x, y):\n",
    "\n",
    "        # Regularizer term\n",
    "        reg = self.lambda_ * (w * w)\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            # Optimization term\n",
    "            opt_term = y[i] * (self.kernel(w, x[i]) + b)\n",
    "\n",
    "            # calculating loss\n",
    "            loss = reg + max(0, 1-opt_term)\n",
    "\n",
    "        return loss[0][0]\n",
    "\n",
    "    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "        # The number of features in X\n",
    "        number_of_features = X.shape[1]\n",
    "\n",
    "        # The number of Samples in X\n",
    "        number_of_samples = X.shape[0]\n",
    "\n",
    "        # Creating ids from 0 to number_of_samples - 1\n",
    "        ids = np.arange(number_of_samples)\n",
    "\n",
    "        # Shuffling the samples randomly\n",
    "        np.random.shuffle(ids)\n",
    "\n",
    "        # Create an array of random numbers in the range [-1, 1] for w\n",
    "        w = np.random.uniform(low=-1, high=1, size=(1, number_of_features))\n",
    "\n",
    "        # Create a random number in the range [-10, 10] for b\n",
    "        b = np.random.uniform(low=-10, high=10)\n",
    "        losses = []\n",
    "\n",
    "        # Gradient Descent logic\n",
    "        for _ in range(epochs):\n",
    "            # Calculating the Hinge Loss\n",
    "            l = self.hingeloss(w, b, X, Y)\n",
    "\n",
    "            # Appending all losses\n",
    "            losses.append(l)\n",
    "\n",
    "            # Starting from 0 to the number of samples with batch_size as interval\n",
    "            for batch_initial in range(0, number_of_samples, batch_size):\n",
    "                gradw = 0\n",
    "                gradb = 0\n",
    "\n",
    "                for j in range(batch_initial, batch_initial + batch_size):\n",
    "                    if j < number_of_samples:\n",
    "                        x = ids[j]\n",
    "                        ti = Y[x] * (self.kernel(w, X[x]) + b)\n",
    "\n",
    "                        if ti > 1:\n",
    "                            gradw += 0\n",
    "                            gradb += 0\n",
    "                        else:\n",
    "                            # Calculating the gradients\n",
    "\n",
    "                            #w.r.t w\n",
    "                            gradw += Y[x] * X[x]\n",
    "                            # w.r.t b\n",
    "                            gradb += Y[x]\n",
    "\n",
    "                # Updating weights and bias\n",
    "                w = w - learning_rate * w + learning_rate * gradw\n",
    "                b = b + learning_rate * gradb\n",
    "\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        return self.w, self.b, losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "        return np.sign(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.547841109663234e-05\n",
      "Prediction: [ 1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      "  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1.\n",
      " -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1. -1.  1.  1.\n",
      "  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.\n",
      "  1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "Confusion Matrix:\n",
      " [[121   0]\n",
      " [  0 179]]\n",
      "w, b: [array([[ 5.09457956e-03,  1.02399616e-02,  6.18483591e-04,\n",
      "         2.49990343e-03, -3.23400241e-03,  1.17643636e-02,\n",
      "         4.29260027e-03,  4.73233062e-03,  6.24692117e-03,\n",
      "         2.98179333e-03,  2.22510418e-03,  4.86576062e-03,\n",
      "        -5.95172284e-03, -4.13412856e-03,  2.16330051e-02,\n",
      "         1.12232332e-02,  5.96171882e-03,  1.03288582e+00]]), -0.02863016628516006]\n"
     ]
    }
   ],
   "source": [
    "input_file = \"dataset.csv\"\n",
    "x_train, x_test, y_train, y_test = initialize_dataset(input_file)\n",
    "svm = SVM(linear_kernel)\n",
    "w, b, losses = svm.fit(x_train, y_train)\n",
    "prediction = svm.predict(x_test)\n",
    "lss = losses.pop()\n",
    "\n",
    "print(\"Loss:\", lss)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Accuracy:\", accuracy_score(prediction, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, prediction))\n",
    "print(\"Recall:\", recall_score(y_test, prediction))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction))\n",
    "print(\"w, b:\", [w, b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
